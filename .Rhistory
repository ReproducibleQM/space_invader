#first half of day
#amplitude of temperature difference
alpha1<-(maxi[i]-mini[i])/2
#average temperature
avg1<-(maxi[i]+mini[i])/2
#theta is time point when temperature crosses the threshold
#assuming temperature is roughly following the sine curve
theta1<-asin((thresh-avg1)/alpha1)
#use these to calculate degree day accumulation over first half of day
dd1.T<-(1/(2*pi))*((avg1-thresh)*(pi/2 - theta1)+alpha1*cos(theta1))
dd1<-c(dd1, dd1.T)
#second half of day
#two possible cases, min temperature on day i+1 could be below thereshold or above
#for below threshold:
if (mini[i+1]<thresh){
#amplitude of temperature difference
alpha2<-(maxi[i]-mini[i+1])/2
#average temperature
avg2<-(maxi[i]+mini[i+1])/2
#theta is time point when temperature crosses the threshold
#assuming temperature is roughly following the sine curve
theta2<-asin((thresh-avg2)/alpha2)
#use these to calculate degree day accumulation over first half of day
dd2.T<-(1/(2*pi))*((avg2-thresh)*(pi/2 - theta2)+alpha2*cos(theta2))
dd2<-c(dd2, dd2.T)
} else { #for above threshold
#second half of day
avg2<-(maxi[i]+mini[i+1])/2
dd2.T<-(avg2-thresh)/2
dd2<-c(dd2, dd2.T)
}
} else if (mini[i]>=thresh){
#first half of day
avg1<-(maxi[i]+mini[i])/2
dd1.T<-(avg1-thresh)/2
dd1<-c(dd1, dd1.T)
#second half of day, as above, two possible cases
if (mini[i+1]>=thresh){
avg2<-(maxi[i]+mini[i+1])/2
dd2.T<-(avg2-thresh)/2
dd2<-c(dd2, dd2.T)
} else{
#amplitude of temperature difference
alpha2<-(maxi[i]-mini[i+1])/2
#average temperature
avg2<-(maxi[i]+mini[i+1])/2
#theta is time point when temperatur crosses the threshold
#assuming temperature is roughly following the sine curve
theta2<-asin((thresh-avg2)/alpha2)
#use these to calculate degree day accumulation over first half of day
dd2.T<-(1/(2*pi))*((avg2-thresh)*(pi/2 - theta2)+alpha2*cos(theta2))
dd2<-c(dd2, dd2.T)
}
}
else  {
#if temperature doesn't get over threshold, no degree days accumulated
#first half of day
dd1<-c(dd1, 0)
#second half of day
dd2<-c(dd2, 0)
}
#total accumulation over the day is just first half of day plus second
}
return(dd1+dd2)
}
#do some checks to make sure the function is working properly
weather$dd<-allen(weather$temp_max_cleaned, weather$temp_min_cleaned, 10)
#plot to make sure nothing weird is happening- look for more degree days midyear,
#and NO negative values. Looks like we're WINNING!
plot(weather$DOY, weather$dd)
#now write a new function to calculate accumulated degree days
accum.allen<-function(maxi, mini, thresh, DOY, startday){
#if startday is not given, assume it's day 1
if(missing(startday)) {
startday<-1
} else {
startday<-startday
}
dd<-allen(maxi, mini, thresh)
dd.accum<-c()
for (i in 1:length(dd)){
#hmm, need a way to sum up over the year, starting anew for each year.
#this should do it
if (DOY[i]==1){
dd.accum.day=0
}
#the accumulation on day i is the degree day accumulation before
#plus the dd accumulated on that day
dd.accum.day<-dd.accum.day+dd[i]
#but if the degdays are accumulating before the startday, we want to forget them
if (DOY[i]<startday){
dd.accum.day=0
}
#add that day's accumulation to the vector
dd.accum<-c(dd.accum, dd.accum.day)
}
return (dd.accum)
}
#same sort of checks. Run the function for our data
start<-1
weather$dd.accum<-accum.allen(weather$temp_max_cleaned, weather$temp_min_cleaned, 10, weather$DOY, start)
#and plot that thing to look for problems:
plot(weather$DOY, weather$dd.accum)
#looks good! victory!!!
#we have good reason to think precipitation may also be important for ladybeetles
#let's use the functions developed for the lampyrid analysis to aggregate some precipitation metrics
accum.precip<-function (precip, week){
precip.acc<-c()
counter<-week[1]
accumulation<-0
for (i in 1:length(precip)){
if(week[i]==counter){
accumulation<-accumulation + precip[i]
}else{
counter<-week[i]
accumulation<-precip[i]
}
precip.acc<-c(precip.acc, accumulation)
}
return(precip.acc)
}
#run the precipitation accumulation function
weather$prec.accum<-accum.precip(weather$precipitation, weather$week)
#looks good! now let's count rainy days
#this is a simple thing, doesn't really need a function to encode for it, but what the heck
#might as well be consistent with how we've handled processing other weather data
#encoding rain days as 0/1 will allow us to simply sum up the number of rainy days for whatever time
#period we like
rainy.days<-function (precip, week){
rainy.days<-c()
for (i in 1:length(precip)){
if(precip[i]>0){
raindays<-1
}else{
raindays<-0
}
rainy.days<-c(rainy.days, raindays)
}
return(rainy.days)
}
#and now the rain day counter
weather$rain.days<-rainy.days(weather$precipitation, weather$week)
#finally, we need to be able to compute the accumulated precipitation over the season from a given timepoint
#another function? I think SO! base this one on the degree day accumulation function
accum.precip.time<-function(precip, DOY, startday){
#if startday is not given, assume it's day 1
if(missing(startday)) {
startday<-1
} else {
startday<-startday
}
prec.accum<-c()
for (i in 1:length(DOY)){
#hmm, need a way to sum up over the year, starting anew for each year.
#this should do it
if (DOY[i]==1){
prec.accum.day=0
}
#the accumulation on day i is the precip accumulation before
#plus the precip accumulated on that day
prec.accum.day<-prec.accum.day+precip[i]
#but if the precip is accumulating before the startday, we want to forget them
if (DOY[i]<startday){
prec.accum.day=0
}
#add that day's accumulation to the vector
prec.accum<-c(prec.accum, prec.accum.day)
}
return (prec.accum)
}
weather$prec.accum.0<-accum.precip.time(weather$precipitation, weather$DOY, start)
#and plot that thing to look for problems:
plot(weather$DOY, weather$prec.accum.0)
#now let's put together a weekly 'weather report'
weather1<-group_by(weather, year, week)
weather_weekly<-summarize(weather1,
mean.prec=mean(precipitation),
rain.days=sum(rain.days),
weekly.precip=max(prec.accum),
yearly.precip.accum=max(prec.accum.0),
max.rainfall=max(precipitation),
mean.temp=mean(temp_mean_cleaned),
min.temp=min(temp_min_cleaned),
max.temp=max(temp_max_cleaned),
weekly.dd=max(dd),
yearly.dd.accum=max(dd.accum),
)
#let's merge in the weather data to the ladybeetle data
#first rename the year column in one of the datasets
lb_weekly1994_culled<-rename(lb_weekly1994_culled, year=Year)
lb_all<-merge(lb_weekly1994_culled, weather_weekly)
#while we're at this, let's make some yearly summary data that will allow us to
#characterize weather by year. Since it looks like seasonality plays a role in within-year
#partitioning (spoilers!) let's get some accumulations at key points in the season- let's do
#week 25, 30, and 35 and get dd accum, precip accum for each year
keypoints<-c(20, 25, 30, 35)
weather_keypoints<-weather_weekly[which(weather_weekly$week  %in% keypoints),]
#cull out the non-accumulated data
weather_keypoints1<-weather_keypoints[,c(1:2, 6, 12)]
#now we need to recast each of the response columns as their own unique responses by week
#dd accum
library(reshape2)
dd.year<-dcast(weather_keypoints1, year~week,
value.var ="yearly.dd.accum",  sum)
colnames(dd.year)<-c("year", "dd20", "dd25", "dd30", "dd35")
#create metrics for DIFFERENCE from last time point too
dd.year$dd25.dif<-dd.year$dd25-dd.year$dd20
dd.year$dd30.dif<-dd.year$dd30-dd.year$dd25
dd.year$dd35.dif<-dd.year$dd35-dd.year$dd30
#precip
precip.year<-dcast(weather_keypoints1, year~week,
value.var ="yearly.precip.accum",  sum)
colnames(precip.year)<-c("year", "precip20", "precip25", "precip30", "precip35")
#create metrics for DIFFERENCE from last time point too
precip.year$precip25.dif<-precip.year$precip25-precip.year$precip20
precip.year$precip30.dif<-precip.year$precip30-precip.year$precip25
precip.year$precip35.dif<-precip.year$precip35-precip.year$precip30
#ok, now we can merge this into a yearly weather summary matrix
weather_yearly<-merge(dd.year, precip.year)
#let's do some quick plots to look at ladybeetles by various environmental parameters
lb_all$pertrap<-lb_all$SumOfADULTS/lb_all$TRAPS
#let's look at these data by week
lb_summary_week<-ggplot(lb_all, aes(x=week, y=pertrap, fill=year))+
#geom_point(pch=21)+
scale_fill_binned()+
geom_smooth(aes(color=as.factor(year)))+
facet_wrap(vars(SPID), nrow=2)
lb_summary_week
#ok, same thing but for degree day accumulation
lb_summary_dd<-ggplot(lb_all, aes(x=yearly.dd.accum, y=pertrap, fill=year))+
#geom_point(pch=21)+
scale_fill_binned()+
geom_smooth(aes(color=as.factor(year)))+
facet_wrap(vars(SPID), nrow=2)
lb_summary_dd
#let's look at trapping frequency and DD
lb_summary_traps<-ggplot(lb_all, aes(x=yearly.dd.accum, y=TRAPS, fill=year))+
#geom_point(pch=21)+
scale_fill_binned()+
geom_smooth(aes(color=as.factor(year)))+
facet_wrap(vars(SPID), nrow=2)
lb_summary_traps
#yikes! What are all those 10 trap observations? Christie to investigate!
#looks like there are a few rare occasions that the LB were sampled twice in one week (Monday, then Friday?)
#offset in models should account for the worst of that.
#let's look at rain days
lb_summary_raindays<-ggplot(lb_all, aes(x=rain.days, y=pertrap, fill=year))+
#geom_point(pch=21)+
scale_fill_binned()+
geom_smooth(aes(color=as.factor(year)))+
facet_wrap(vars(SPID), nrow=2)
lb_summary_raindays
#let's look at mean temp
lb_summary_meantemp<-ggplot(lb_all, aes(x=mean.temp, y=pertrap, fill=year))+
#geom_point(pch=21)+
scale_fill_binned()+
geom_smooth(aes(color=as.factor(year)))+
facet_wrap(vars(SPID), nrow=2)
lb_summary_meantemp
#let's look at min temp
lb_summary_mintemp<-ggplot(lb_all, aes(x=min.temp, y=pertrap, fill=year))+
#geom_point(pch=21)+
scale_fill_binned()+
geom_smooth(aes(color=as.factor(year)))+
facet_wrap(vars(SPID), nrow=2)
lb_summary_mintemp
#let's look at max rainfall
lb_summary_maxrainfall<-ggplot(lb_all, aes(x=max.rainfall, y=pertrap, fill=year))+
#geom_point(pch=21)+
scale_fill_binned()+
geom_smooth(aes(color=as.factor(year)))+
facet_wrap(vars(SPID), nrow=2)
lb_summary_maxrainfall
#let's look at some ordination- we'll visualize and conduct analyses to describe how
# the two species are using space, over time.
library(reshape2)
library(vegan)
#create a matrix of observations by community
#create parallel yearly and weekly analyses
landscape.year<-dcast(lb_all, year+REPLICATE+SPID~HABITAT,
value.var ="SumOfADULTS",  sum)
landscape.week<-dcast(lb_all, year+week+SPID~HABITAT,
value.var ="SumOfADULTS",  sum)
#because we have some rep by week combinations with zero observations, we must remove them prior to analysis
landscape.week.1<-landscape.week[rowSums(landscape.week[4:12])>2,]
#strip out the context- yes I know! this seems counter-intuitive and awful
#but vegan (and most community analysis packages) want your response variable as its own object
com.matrix.year<-landscape.year[,4:12]
com.matrix.week<-landscape.week.1[,4:12]
#set up ordination with year data
ord.year<-metaMDS(com.matrix.year, autotransform=TRUE)
ord.year
plot(ord.year, disp='sites', type='n')
points(ord.year, display="sites", select=which(landscape.year$SPID=="HAXY"), pch=19, col="orange")
points(ord.year, display="sites", select=which(landscape.year$SPID=="C7"), pch=15, col="red")
ordilabel(ord.year, display="species", cex=0.75, col="black")
#bring the relevant environmental data back into our enviromental frame
yearly.context<-merge(landscape.year, weather_yearly, all.x = T)
#is the spatiotemporal distribution of harmonia different from that of C7 over years?
#we will do a permanova to check
specmod.y<-adonis(com.matrix.year~SPID, data=landscape.year, method="bray")
specmod.y
fit.year<-envfit(ord.year~year+dd35.dif+
precip35.dif, data=yearly.context, perm=999)
summary(fit.year)
fit.year
plot(fit.year)
#save to pdf
pdf("plots/NMDS_yearly.pdf", height=6, width=6)
plot(ord.year, disp='sites', type='n')
points(ord.year, display="sites", select=which(landscape.year$SPID=="HAXY"), pch=19, cex=0.5,col="orange")
points(ord.year, display="sites", select=which(landscape.year$SPID=="C7"), pch=15, cex=0.5, col="red")
plot(fit.year)
ordilabel(ord.year, display="species", cex=0.75, col="black")
dev.off()
#and now for week
ord.week<-metaMDS(com.matrix.week, autotransform=TRUE)
ord.week
plot(ord.week, disp='sites', type='n')
points(ord.week, display="sites", select=which(landscape.week.1$SPID=="HAXY"), pch=19, cex=0.5,col="orange")
points(ord.week, display="sites", select=which(landscape.week.1$SPID=="C7"), pch=15, cex=0.5, col="red")
ordilabel(ord.week, display="species", cex=0.75, col="black")
#bring the relevant environmental data back into our enviromental frame
weekly.context<-merge(landscape.week.1, weather_weekly, all.x = T)
#is the spatiotemporal distribution of harmonia different from that of C7?
#we will do a permanova to check
specmod<-adonis(com.matrix.week~SPID, data=landscape.week.1, method="bray")
specmod
#we're performing a model selection, using backwards selection from all environmental variables
#we're using the P value and R square, and paying attention to which variables seem too colinear to include
fit.week<-envfit(ord.week~year+
yearly.precip.accum+yearly.dd.accum,
data=weekly.context, perm=999)
summary(fit.week)
fit.week
plot(fit.week)
#save to pdf
pdf("plots/NMDS_weekly.pdf", height=6, width=6)
plot(ord.week, disp='sites', type='n')
points(ord.week, display="sites", select=which(landscape.week.1$SPID=="HAXY"), pch=19, cex=0.5,col="orange")
points(ord.week, display="sites", select=which(landscape.week.1$SPID=="C7"), pch=15, cex=0.5, col="red")
ordilabel(ord.week, display="species", cex=0.75, col="black")
plot(fit.week)
dev.off()
# let's rough in our gam models. Just like with the multivariate analysis, we'll look at
#two different scales- within year dynamics and between year dynamics
library(mgcv)
library(visreg)
#start withe the drivers of within-year variation
gam_lb<-gam(SumOfADULTS~s(yearly.dd.accum, by=as.factor(SPID), sp=1)+
s(rain.days, by=as.factor(SPID), sp=1, k=3)+
HABITAT*SPID+
offset(log(TRAPS)), data=lb_all, family="poisson")
summary(gam_lb)
#check concurvity
concurvity(gam_lb)
#looks fine, sweet!
#let's visualize this!
visreg(gam_lb, "yearly.dd.accum", "SPID", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
visreg(gam_lb, "rain.days", "SPID", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
visreg(gam_lb, "HABITAT","SPID", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
#we'll want to extract the data associated with activity peaks
#ok, I think we found the method we should use! here's the tutorial:
# https://fromthebottomoftheheap.net/2014/05/15/identifying-periods-of-change-with-gams/
#first we create a new dataframe that we can use our model to predict the values for
#create data for C7, holding everything constant but degree days
newData.C7 <- with(lb_all,
data.frame(yearly.dd.accum = seq(0, 1500, length = 300),
TRAPS=5,
year=mean(year),
rain.days=median(rain.days),
SPID="C7",
HABITAT="alfalfa"))
#make the same frame but for 1 more degday
newData.C7.1<- with(lb_all,
data.frame(yearly.dd.accum = seq(1, 1501, length = 300),
TRAPS=5,
year=mean(year),
rain.days=median(rain.days),
SPID="C7",
HABITAT="alfalfa"))
#make predictions
predict.dd.C7<-predict(gam_lb, newData.C7, type="link")
predict.dd.C7.1<-predict(gam_lb, newData.C7.1, type="link")
dd.C7.der<-as.data.frame(cbind(newData.C7$yearly.dd.accum, predict.dd.C7, predict.dd.C7.1))
dd.C7.der$slope<-(dd.C7.der$predict.dd.C7.1-dd.C7.der$predict.dd.C7)/1
#then let's do the same thing with harmonia
newData.HAXY <- with(lb_all,
data.frame(yearly.dd.accum = seq(0, 1500, length = 300),
TRAPS=5,
year=mean(year),
rain.days=median(rain.days),
SPID="HAXY",
HABITAT="alfalfa"))
#make the same frame but for 1 more degday
newData.HAXY.1<- with(lb_all,
data.frame(yearly.dd.accum = seq(1, 1501, length = 300),
TRAPS=5,
year=mean(year),
rain.days=median(rain.days),
SPID="HAXY",
HABITAT="alfalfa"))
#make predictions
predict.dd.HAXY<-predict(gam_lb, newData.HAXY, type="link")
predict.dd.HAXY.1<-predict(gam_lb, newData.HAXY.1, type="link")
dd.HAXY.der<-as.data.frame(cbind(newData.HAXY$yearly.dd.accum, predict.dd.HAXY, predict.dd.HAXY.1))
dd.HAXY.der$slope<-(dd.HAXY.der$predict.dd.HAXY.1-dd.HAXY.der$predict.dd.HAXY)/1
#Start of population growth dd.accum
#Peak population growth dd.accum
#other inflection points dd.accum
#look at the shape of the model for other environmental variables
#box plot of residuals by species and habitat
#now that we have a reasonable model for within-year variation,
#let's see how much of the year-to-year variation is explained by within season trends
#use the same model a above but now let's explicitly look for year-to-year variability
gam_lb_yeartrend<-gam(SumOfADULTS~s(yearly.dd.accum, by=as.factor(SPID), sp=1, k=6)+
s(rain.days, by=as.factor(SPID), sp=1, k=3)+
HABITAT*SPID+
s(year, by=as.factor(SPID), sp=1)+
offset(log(TRAPS)), data=lb_all, family="poisson")
summary(gam_lb_yeartrend)
#check concurvity
concurvity(gam_lb_yeartrend)
#looks fine, sweet!
visreg(gam_lb_yeartrend, "year", "SPID", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
#now, what accounts for the year to-year variation in absolute numbers of both species?
#let's do another gam, but with the yearly aggregated data and the summary weather metrics
lb_yearly<-rename(lb_yearly, year=Year)
#also because we're looking for drivers, let's re-arrange the data so one beetle can
#be considered as a driver for the other
lb_yearly_1<-dcast(lb_yearly, year+REPLICATE+TREAT+HABITAT~SPID,
value.var ="SumOfADULTS",  sum)
lb_yearly_2<-merge(lb_yearly_1,unique(lb_yearly[,c(1:4,7)]), all.x=T)
lb_yearly_weather<-merge(lb_yearly_2, weather_yearly, all.x=T)
#another gam using this data aggregated by year but not by rep, and because the
#species seem to be responding to different factors, let's do one at a time
gam_haxy_yearly<-gam(HAXY~s(C7, sp=1, k=4)+
#s(dd20, sp=1, k=4)+
s(dd25.dif, sp=1, k=4)+
s(dd30.dif, sp=1, k=4)+
#s(dd35.dif, sp=1, k=4)+
s(precip20, sp=1, k=4)+
#s(precip25.dif, sp=1, k=4)+
#s(precip30.dif, sp=1, k=4)+
s(precip35.dif, sp=1, k=4)+
HABITAT+
offset(log(TRAPS)), data=lb_yearly_weather, family="quasipoisson")
summary(gam_haxy_yearly)
#because the model has a lot of variables that are probably a bit autocorrelated,
#check concurvity to see if it needs simplification- aim to get observed >0.5 for all values
concurvity(gam_haxy_yearly)
#eliminate variables with least explanatory power (Lower F) from set with high concurvity
# start with precip25.dif,then dd20, then dd35.dif, then precip30.dif
visreg(gam_haxy_yearly, "C7", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
# visreg(gam_haxy_yearly, "dd20", partial=FALSE, rug=FALSE,
#        overlay=TRUE, scale="response")
visreg(gam_haxy_yearly, "dd25.dif", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
visreg(gam_haxy_yearly, "dd30.dif", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
# visreg(gam_haxy_yearly, "dd35.dif", partial=FALSE, rug=FALSE,
#        overlay=TRUE, scale="response")
visreg(gam_haxy_yearly, "precip20", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
# visreg(gam_haxy_yearly, "precip25.dif", partial=FALSE, rug=FALSE,
#        overlay=TRUE, scale="response")
# visreg(gam_haxy_yearly, "precip30.dif", partial=FALSE, rug=FALSE,
#        overlay=TRUE)
visreg(gam_haxy_yearly, "precip35.dif", partial=FALSE, rug=FALSE,
overlay=TRUE)
##### now C7
gam_c7_yearly<-gam(C7~s(HAXY, sp=1, k=4)+
s(dd20, sp=1, k=4)+
#s(dd25.dif, sp=1, k=4)+
#s(dd30.dif, sp=1, k=4)+
s(dd35.dif, sp=1, k=4)+
#s(precip20, sp=1, k=4)+
s(precip25.dif, sp=1, k=4)+
#s(precip30.dif, sp=1, k=4)+
#s(precip35.dif, sp=1, k=4)+
HABITAT+
offset(log(TRAPS)), data=lb_yearly_weather, family="quasipoisson")
summary(gam_c7_yearly)
#because the model has a lot of variables that are probably a bit autocorrelated,
#check concurvity to see if it needs simplification- aim to get observed >0.5 for all values
concurvity(gam_c7_yearly)
#eliminate variables with least explanatory power (Lower F) from set with high concurvity
# start with dd25.dif, dd30.dif, then precip20, precip30, precip35
visreg(gam_c7_yearly, "HAXY", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
visreg(gam_c7_yearly, "dd20", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
# visreg(gam_c7_yearly, "dd25.dif", partial=FALSE, rug=FALSE,
#        overlay=TRUE, scale="response")
#
# visreg(gam_c7_yearly, "dd30.dif", partial=FALSE, rug=FALSE,
#        overlay=TRUE, scale="response")
visreg(gam_c7_yearly, "dd35.dif", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
# visreg(gam_c7_yearly, "precip20", partial=FALSE, rug=FALSE,
#        overlay=TRUE, scale="response")
visreg(gam_c7_yearly, "precip25.dif", partial=FALSE, rug=FALSE,
overlay=TRUE, scale="response")
# visreg(gam_c7_yearly, "precip30.dif", partial=FALSE, rug=FALSE,
#        overlay=TRUE)
# visreg(gam_c7_yearly, "precip35.dif", partial=FALSE, rug=FALSE,
#        overlay=TRUE)
