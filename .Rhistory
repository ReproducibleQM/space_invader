# septempunctata and Harmonia axyridis niche partitioning
#bring data into R
LB<-read.csv(file="data/KBS_Haxy_C7_1989-2020.csv", header=T,
na.strings=c(NA))
#### some silly functions, conditionals and loops to demonstrate their structure in R
sum.of.squares <- function(x,y) {#paste in demo function that calculates sum of squares
x^2 + y^2 #add square of x to square of y
}
sum.of.squares(3,4)#demo sum of squares function
iszero<-function(number){#create a function that determines if a value is zero
if(number==0){
print("yes")#if the number is zero, print yes
}else{
print("no")#otherwise print no
}
}
iszero("cherry pie")#demonstrate iszero function
vec<-c(1,2,3,0)#define a test vector for demonstrating a function on a loop
for(i in 1:length(vec)){#demonstrate a simple loop
iszero(vec[i])#is each item in the vector zero?
}
##### enough of all that, what about real data?
summary(LB)
#clean data
#first, we fix dates, make them ISO'ed
library(lubridate)
LB$newdate<-mdy(LB$DATE)#parses the date format used for the forest plots
LB$newdate<-mdy_hm(LB$DATE)#parses the date format used in the main plots
#well, crap, neither command gets all of the dates to parse correctly
#we have an issue because data from the main site exported with time stamps but
#the forest site did not. Ugh.
#Christie's solution: brute force removal of timestamps
LB$DATE<-gsub(" 0:00", "", LB$DATE)#remove time stamp strings
LB$newdate<-mdy(LB$DATE)#parses the date format now used by all observations
summary(LB)#bingo! looks like it worked!
###################################
#UNADAPTED PASTED CODE BELOW WILL NOT WORK
#download weather data from KBS weather station
weather<-read.table(file="http://lter.kbs.msu.edu/datatables/7.csv",
header=T, sep=",", na.strings="")
#extract day of year, so we have a continuous variable running for each year.
#since we're in a temperate northern climate, this is convenient- not too
#much insect action happening at the december-january transition, so we
#can use the yearly break as a blocking variable for rowing season.
#it's convenient living where we do!
weather$DOY<-yday(weather$date)
weather$week<-isoweek(weather$date)
#do a few simple plots to make sure the data makes sense -this is
#a good way to check that the importation was successful
plot(weather$DOY, weather$air_temp_mean)
plot(weather$DOY, weather$precipitation)
#let's cut out the data from before 1989 so we can process the weather data more quickly. Al so we'll cut off the weather
#data that's causing us problems- we don't need it anyway
weather<-subset(weather, weather$year>=1989& weather$year<=2020)
#lets also get rid of the variables we don't need:
weather$flag_precip<-NULL
weather$flag_air_temp_mean<-NULL
weather$flag_air_temp_max<-NULL
weather$flag_air_temp_min<-NULL
#also, these data are sorted in decending order. It's easier to think of this
#stuff in ascending order, so let's sort the data by year and DOY
weather<-weather[order(weather$year, weather$DOY),]
#Let's examine the data to see how complete it is
summary(weather)
#lets's pre-process these weather data so we get rid of missing values
# we can write a function to do this for us.
#if missing data is rare, it is probably safe to assume that missing
#temperatures are similar to the weather on the day before or after.
#for the sake of simplicity, let's replace a missing value with the
#value for that variable for the day before
#first, define the function
replace.missing<-function(vec){
#create a vector to put our new values into
New = c()
for (i in 1:(length(vec))){
if (is.na(vec[i])){
vec[i]<-vec[i-1]
#if the data is missing, sub in the value from the measurement before
} else{
#if the value is not missing, just pass it through to the result vector
vec[i]<-vec[i]
}
New=c(New, vec[i])
}
if (any(is.na(New))){
replace.missing(New)
}
return(New)
}
View(weather)
View(weather)
download weather data from KBS weather station
weather<-read.table(file="http://lter.kbs.msu.edu/datatables/7.csv",
header=T, sep=",", na.strings="")
View(weather)
View(weather)
## **Data Reuse Plan**
This is the basic metadata file for our project dataset, also housed in this repository.
## | **What** |
**Description (abstract):** This project is about niche differentiation of lady beetles at Kellogg Biological Station (KBS), South-western Michigan, United States. The major goal of this project is to analyse long term ecological data collected since 1989 on two species of lady beetles. This project is a part of a graduate coursework in Reproducible Quantitative Methods (RQM) at Kent State University.
**Title:** Coccinella septempunctata and Harmonia axyridis caught on sticky traps in the Main Cropping System Experiment, Kellogg Biological Station, 1989-2020
**Permanent Identifier:**
http://doi.org/10.6073/pasta/6b6cc0ad7897d9008e8cf918bbf552d2
Original data landing page: https://lter.kbs.msu.edu/datatables/67
**Data Source:** Data extracted from KBS LTER Main Cropping System Experiment
KBS LTER Datatable - Insect Populations via Sticky Traps (msu.edu)
**Subject:** Entomology, community ecology, environmental science, agriculture, invasive species, insects
**Related publications:** Have you published an article, thesis or some other publication based on this data? Include a full citation and permanent identifier, if available.
https://doi.org/10.1371/journal.pcbi.1007542
10.1371/journal.pone.0083407
10.1007/s10530-014-0772-4
http://dx.doi.org/10.1890/14-2022.1
## | **Who** |
KBS LTER, Michigan State University; Christie Bahlai, PhD, Kent State University
**Funder Information** : Support for this research was also provided by the NSF Long-term Ecological Research Program (DEB 1832042) at the Kellogg Biological Station and by Michigan State University AgBioResearch. Additional funding has come from US Department of Energy, US Department of Agriculture, and Electrical Power Research, the BEACON Center for Evolution in Action, and the Mozilla Foundation.
**Collaborators:** Christie Bahlai, Erin VanderJeugdt, Matthew B. Arnold, Omon Obarein, Prashant Ghimire, Nageen Farooq, Michael Back, Kyle Smart, Trixie Taucher, Mike Crowell. Affiliations include: Kent State University, Michigan State University
**Contact person:**
Christie Bahlai, Investigator, cbahlai@kent.edu
Doug Landis, Lead Investigator, landisd@msu.edu
## | **Where** |
**Location:** Kellogg Biological Station’s Main Cropping System Experiment, and adjacent forest plots, South West Michigan.
## | **When** |
**Temporal Coverage:** 1989-05-24 to 2020-09-04. Sampling is conducted weekly during the growing season as described in the sampling protocol. Sampling periods varied from 8-15 weeks in a given year depending on crop management issues and labor availability.
**Publication Date:** When was the data made available in the place of publication (above)?
Most recent addition: 2020-09-04 (we have access to newer data, pre-publication)
## | **How** |
**Data collection process:** The standard method used to measure these organisms is a yellow sticky trap. Insects collected on sticky-trap are classified by species,family and order. The insects are counted for each treatment, replicate and station combination. The actual sticky trap location coordinates are given in UTM 16N.
Protocol - Insect Abundance - Sticky Traps (msu.edu)
**Data processing:** How did you clean the data? How are missing or null values handled? Did you write code for processing the data and where can it be found?
**File index:**
See https://github.com/ReproducibleQM/space_invader/tree/main/data
**File format/s:** Dataset saved as text file in the comma separated values (.csv) format. This file can be viewed in any text editor, or any spreadsheet software.
## Variables that appear in these data:
Year: The year the sample was collected from 1989-2020.
DATE: The date the sample was collected in mm/dd/yyyy format.
TREAT: Treatment ID of plot from 1-7, plus three forest treatments, SF, DF, and CF.
Possible values: T1- Field crop rotation, Conventional management
T2- Field crop rotation, no-till management
T3- Reduced input crop rotation with cover crop
T4- Biologically based crop rotation with cover crop
T5- Poplar (plated in 2019)
T6- Switchgrass (Alfalfa from 19
T7- Early successional community
HABITAT: The predominant plant or plant community present  in the plot, in a given year. Note that the original data file contains spelling errors that are resolved by substitution in the data cleaning script.
Maize, soybean, wheat, early successional, poplar, alfalfa, switchgrass, successional forest, coniferous forest, deciduous forest.
REPLICATE: Number identifiers for replicates of treatment plots. There are six replicates of the agricultural treatments T1-T7, and three replicates each of the surrounding forest plots, SF, CF, and DF
STATION: Sampling station within each plot, from five permanent sampling stations.
SPID: Species Identification: C7 for Coccinella septempunctata and HAXY for Harmonia axyridis.
SumOfADULTS: number of adults of that species that was in the trap at sampling time
## **Data Reuse Plan**
This is the basic metadata file for our project dataset, also housed in this repository.
## | **What** |
**Description (abstract):** This project is about niche differentiation of lady beetles at Kellogg Biological Station (KBS), South-western Michigan, United States. The major goal of this project is to analyse long term ecological data collected since 1989 on two species of lady beetles. This project is a part of a graduate coursework in Reproducible Quantitative Methods (RQM) at Kent State University.
**Title:** Coccinella septempunctata and Harmonia axyridis caught on sticky traps in the Main Cropping System Experiment, Kellogg Biological Station, 1989-2020
**Permanent Identifier:**
http://doi.org/10.6073/pasta/6b6cc0ad7897d9008e8cf918bbf552d2
Original data landing page: https://lter.kbs.msu.edu/datatables/67
**Data Source:** Data extracted from KBS LTER Main Cropping System Experiment
KBS LTER Datatable - Insect Populations via Sticky Traps (msu.edu)
**Subject:** Entomology, community ecology, environmental science, agriculture, invasive species, insects
**Related publications:** Have you published an article, thesis or some other publication based on this data? Include a full citation and permanent identifier, if available.
https://doi.org/10.1371/journal.pcbi.1007542
10.1371/journal.pone.0083407
10.1007/s10530-014-0772-4
http://dx.doi.org/10.1890/14-2022.1
## | **Who** |
KBS LTER, Michigan State University; Christie Bahlai, PhD, Kent State University
**Funder Information** : Support for this research was also provided by the NSF Long-term Ecological Research Program (DEB 1832042) at the Kellogg Biological Station and by Michigan State University AgBioResearch. Additional funding has come from US Department of Energy, US Department of Agriculture, and Electrical Power Research, the BEACON Center for Evolution in Action, and the Mozilla Foundation.
**Collaborators:** Christie Bahlai, Erin VanderJeugdt, Matthew B. Arnold, Omon Obarein, Prashant Ghimire, Nageen Farooq, Michael Back, Kyle Smart, Trixie Taucher, Mike Crowell. Affiliations include: Kent State University, Michigan State University
**Contact person:**
Christie Bahlai, Investigator, cbahlai@kent.edu
Doug Landis, Lead Investigator, landisd@msu.edu
## | **Where** |
**Location:** Kellogg Biological Station’s Main Cropping System Experiment, and adjacent forest plots, South West Michigan.
## | **When** |
**Temporal Coverage:** 1989-05-24 to 2020-09-04. Sampling is conducted weekly during the growing season as described in the sampling protocol. Sampling periods varied from 8-15 weeks in a given year depending on crop management issues and labor availability.
**Publication Date:** When was the data made available in the place of publication (above)?
Most recent addition: 2020-09-04 (we have access to newer data, pre-publication)
## | **How** |
**Data collection process:** The standard method used to measure these organisms is a yellow sticky trap. Insects collected on sticky-trap are classified by species,family and order. The insects are counted for each treatment, replicate and station combination. The actual sticky trap location coordinates are given in UTM 16N.
Protocol - Insect Abundance - Sticky Traps (msu.edu)
**Data processing:** How did you clean the data? How are missing or null values handled? Did you write code for processing the data and where can it be found?
**File index:**
See https://github.com/ReproducibleQM/space_invader/tree/main/data
**File format/s:** Dataset saved as text file in the comma separated values (.csv) format. This file can be viewed in any text editor, or any spreadsheet software.
## Variables that appear in these data:
Year: The year the sample was collected from 1989-2020.
DATE: The date the sample was collected in mm/dd/yyyy format.
TREAT: Treatment ID of plot from 1-7, plus three forest treatments, SF, DF, and CF.
Possible values: T1- Field crop rotation, Conventional management
T2- Field crop rotation, no-till management
T3- Reduced input crop rotation with cover crop
T4- Biologically based crop rotation with cover crop
T5- Poplar (plated in 2019)
T6- Switchgrass (Alfalfa from 19
T7- Early successional community
HABITAT: The predominant plant or plant community present  in the plot, in a given year. Note that the original data file contains spelling errors that are resolved by substitution in the data cleaning script.
Maize, soybean, wheat, early successional, poplar, alfalfa, switchgrass, successional forest, coniferous forest, deciduous forest.
REPLICATE: Number identifiers for replicates of treatment plots. There are six replicates of the agricultural treatments T1-T7, and three replicates each of the surrounding forest plots, SF, CF, and DF
STATION: Sampling station within each plot, from five permanent sampling stations.
SPID: Species Identification: C7 for Coccinella septempunctata and HAXY for Harmonia axyridis.
SumOfADULTS: number of adults of that species that was in the trap at sampling time
# Main analysis of KBS Ladybeetle 2020 data, Coccinella
# septempunctata and Harmonia axyridis niche partitioning
#bring data into R
LB<-read.csv(file="data/KBS_Haxy_C7_1989-2020.csv", header=T,
na.strings=c(NA))
#### some silly functions, conditionals and loops to demonstrate their structure in R
sum.of.squares <- function(x,y) {#paste in demo function that calculates sum of squares
x^2 + y^2 #add square of x to square of y
}
sum.of.squares(3,4)#demo sum of squares function
iszero<-function(number){#create a function that determines if a value is zero
if(number==0){
print("yes")#if the number is zero, print yes
}else{
print("no")#otherwise print no
}
}
iszero("cherry pie")#demonstrate iszero function
vec<-c(1,2,3,0)#define a test vector for demonstrating a function on a loop
for(i in 1:length(vec)){#demonstrate a simple loop
iszero(vec[i])#is each item in the vector zero?
}
##### enough of all that, what about real data?
summary(LB)
#clean data
#first, we fix dates, make them ISO'ed
library(lubridate)
LB$newdate<-mdy(LB$DATE)#parses the date format used for the forest plots
LB$newdate<-mdy_hm(LB$DATE)#parses the date format used in the main plots
#well, crap, neither command gets all of the dates to parse correctly
#we have an issue because data from the main site exported with time stamps but
#the forest site did not. Ugh.
#Christie's solution: brute force removal of timestamps
LB$DATE<-gsub(" 0:00", "", LB$DATE)#remove time stamp strings
LB$newdate<-mdy(LB$DATE)#parses the date format now used by all observations
summary(LB)#bingo! looks like it worked!
###################################
#UNADAPTED PASTED CODE BELOW WILL NOT WORK
#download weather data from KBS weather station
weather<-read.table(file="http://lter.kbs.msu.edu/datatables/7.csv",
header=T, sep=",", na.strings="")
#extract day of year, so we have a continuous variable running for each year.
#since we're in a temperate northern climate, this is convenient- not too
#much insect action happening at the december-january transition, so we
#can use the yearly break as a blocking variable for rowing season.
#it's convenient living where we do!
weather$DOY<-yday(weather$date)
weather$week<-isoweek(weather$date)
#do a few simple plots to make sure the data makes sense -this is
#a good way to check that the importation was successful
plot(weather$DOY, weather$air_temp_mean)
plot(weather$DOY, weather$precipitation)
#let's cut out the data from before 1989 so we can process the weather data more quickly. Al so we'll cut off the weather
#data that's causing us problems- we don't need it anyway
weather<-subset(weather, weather$year>=1989& weather$year<=2020)
#lets also get rid of the variables we don't need:
weather$flag_precip<-NULL
weather$flag_air_temp_mean<-NULL
weather$flag_air_temp_max<-NULL
weather$flag_air_temp_min<-NULL
#also, these data are sorted in decending order. It's easier to think of this
#stuff in ascending order, so let's sort the data by year and DOY
weather<-weather[order(weather$year, weather$DOY),]
#Let's examine the data to see how complete it is
summary(weather)
#lets's pre-process these weather data so we get rid of missing values
# we can write a function to do this for us.
#if missing data is rare, it is probably safe to assume that missing
#temperatures are similar to the weather on the day before or after.
#for the sake of simplicity, let's replace a missing value with the
#value for that variable for the day before
#first, define the function
replace.missing<-function(vec){
#create a vector to put our new values into
New = c()
for (i in 1:(length(vec))){
if (is.na(vec[i])){
vec[i]<-vec[i-1]
#if the data is missing, sub in the value from the measurement before
} else{
#if the value is not missing, just pass it through to the result vector
vec[i]<-vec[i]
}
New=c(New, vec[i])
}
if (any(is.na(New))){
replace.missing(New)
}
return(New)
}
# Main analysis of KBS Ladybeetle 2020 data, Coccinella
# septempunctata and Harmonia axyridis niche partitioning
#bring data into R
LB<-read.csv(file="data/KBS_Haxy_C7_1989-2020.csv", header=T,
na.strings=c(NA))
##### enough of all that, what about real data?
summary(LB)
#clean data
#first, we fix dates, make them ISO'ed
library(lubridate)
LB$newdate<-mdy(LB$DATE)#parses the date format used for the forest plots
LB$newdate<-mdy_hm(LB$DATE)#parses the date format used in the main plots
#well, crap, neither command gets all of the dates to parse correctly
#we have an issue because data from the main site exported with time stamps but
#the forest site did not. Ugh.
#Christie's solution: brute force removal of timestamps
LB$DATE<-gsub(" 0:00", "", LB$DATE)#remove time stamp strings
LB$newdate<-mdy(LB$DATE)#parses the date format now used by all observations
summary(LB)#bingo! looks like it worked!
###################################
#Begin weather data processing
#download weather data from KBS weather station
#download a local copy into the data folder and then pull it from there to load
# # how file was downloaded- commented out so it's not run and re-downloaded each time
# # Specify URL where file is stored
# url <- "http://lter.kbs.msu.edu/datatables/7.csv"
# # Specify destination where file should be saved
# destfile <- "data/kbsweather.csv"
# # Apply download.file function in R
# download.file(url, destfile)
weather<-read.csv(file="data/kbsweather.csv",
header=T, sep=",", na.strings="", comment.char = '#')
#extract day of year, so we have a continuous variable running for each year.
#since we're in a temperate northern climate, this is convenient- not too
#much insect action happening at the december-january transition, so we
#can use the yearly break as a blocking variable for rowing season.
#it's convenient living where we do!
weather$DOY<-yday(weather$date)
weather$week<-isoweek(weather$date)
#do a few simple plots to make sure the data makes sense -this is
#a good way to check that the importation was successful
plot(weather$DOY, weather$air_temp_mean)
plot(weather$DOY, weather$precipitation)
#let's cut out the data from before 1989 so we can process the weather data more quickly. Al so we'll cut off the weather
#data that's causing us problems- we don't need it anyway
weather<-subset(weather, weather$year>=1989& weather$year<=2020)
#lets also get rid of the variables we don't need:
weather$flag_precip<-NULL
weather$flag_air_temp_mean<-NULL
weather$flag_air_temp_max<-NULL
weather$flag_air_temp_min<-NULL
#also, these data are sorted in descending order. It's easier to think of this
#stuff in ascending order, so let's sort the data by year and DOY
weather<-weather[order(weather$year, weather$DOY),]
#Let's examine the data to see how complete it is
summary(weather)
#let's pre-process these weather data so we get rid of missing values
# we can write a function to do this for us.
#if missing data is rare, it is probably safe to assume that missing
#temperatures are similar to the weather on the day before or after.
#for the sake of simplicity, let's replace a missing value with the
#value for that variable for the day before
#first, define the function
replace.missing<-function(vec){
#create a vector to put our new values into
New = c()
for (i in 1:(length(vec))){
if (is.na(vec[i])){
vec[i]<-mean(c(vec[i-1], vec[i+1]), na.rm=TRUE)
#if the data is missing, sub in the value from the measurement before
} else{
#if the value is not missing, just pass it through to the result vector
vec[i]<-vec[i]
}
New=c(New, vec[i])
}
if (any(is.na(New))){
replace.missing(New)
}
return(New)
}
#now let's use our replace missing function to gap fill our weather data
weather$temp_mean_cleaned<-replace.missing(weather$air_temp_mean)
weather$temp_min_cleaned<-replace.missing(weather$air_temp_min)
weather$temp_max_cleaned<-replace.missing(weather$air_temp_max)
# calculate the degree day accumulation for the first half of the day dd1,
#assuming a sine wave structure of temperature over the day
#use a development threshold of 10C, well, because it's a nice number
#to work with
#we'll use the model presented in Allen 1976 which uses daily max and min temperatures
#and assumes temperature follows a sine wave
allen<-function(maxi, mini, thresh){
#if threshold is not given, assume it's 10 Celcius
if(missing(thresh)) {
thresh<-10
} else {
thresh<-thresh
}
dd1<-c()
dd2<-c()
for (i in 1:length(maxi)){
if (maxi[i]>= thresh & mini[i]<thresh) {
#first half of day
#amplitude of temperature difference
alpha1<-(maxi[i]-mini[i])/2
#average temperature
avg1<-(maxi[i]+mini[i])/2
#theta is time point when temperature crosses the threshold
#assuming temperature is roughly following the sine curve
theta1<-asin((thresh-avg1)/alpha1)
#use these to calculate degree day accumulation over first half of day
dd1.T<-(1/(2*pi))*((avg1-thresh)*(pi/2 - theta1)+alpha1*cos(theta1))
dd1<-c(dd1, dd1.T)
#second half of day
#two possible cases, min temperature on day i+1 could be below thereshold or above
#for below threshold:
if (mini[i+1]<thresh){
#amplitude of temperature difference
alpha2<-(maxi[i]-mini[i+1])/2
#average temperature
avg2<-(maxi[i]+mini[i+1])/2
#theta is time point when temperature crosses the threshold
#assuming temperature is roughly following the sine curve
theta2<-asin((thresh-avg2)/alpha2)
#use these to calculate degree day accumulation over first half of day
dd2.T<-(1/(2*pi))*((avg2-thresh)*(pi/2 - theta2)+alpha2*cos(theta2))
dd2<-c(dd2, dd2.T)
} else { #for above threshold
#second half of day
avg2<-(maxi[i]+mini[i+1])/2
dd2.T<-(avg2-thresh)/2
dd2<-c(dd2, dd2.T)
}
} else if (mini[i]>=thresh){
#first half of day
avg1<-(maxi[i]+mini[i])/2
dd1.T<-(avg1-thresh)/2
dd1<-c(dd1, dd1.T)
#second half of day, as above, two possible cases
if (mini[i+1]>=thresh){
avg2<-(maxi[i]+mini[i+1])/2
dd2.T<-(avg2-thresh)/2
dd2<-c(dd2, dd2.T)
} else{
#amplitude of temperature difference
alpha2<-(maxi[i]-mini[i+1])/2
#average temperature
avg2<-(maxi[i]+mini[i+1])/2
#theta is time point when temperatur crosses the threshold
#assuming temperature is roughly following the sine curve
theta2<-asin((thresh-avg2)/alpha2)
#use these to calculate degree day accumulation over first half of day
dd2.T<-(1/(2*pi))*((avg2-thresh)*(pi/2 - theta2)+alpha2*cos(theta2))
dd2<-c(dd2, dd2.T)
}
}
else  {
#if temperature doesn't get over threshold, no degree days accumulated
#first half of day
dd1<-c(dd1, 0)
#second half of day
dd2<-c(dd2, 0)
}
#total accumulation over the day is just first half of day plus second
}
return(dd1+dd2)
}
#do some checks to make sure the function is working properly
weather$dd<-allen(weather$temp_max_cleaned, weather$temp_min_cleaned, 10)
#plot to make sure nothing weird is happening- look for more degree days midyear,
#and NO negative values. Looks like we're WINNING!
plot(weather$DOY, weather$dd)
accum.allen<-function(maxi, mini, thresh, DOY, startday){
#if startday is not given, assume it's day 1
if(missing(startday)) {
startday<-1
} else {
startday<-startday
}
dd<-allen(maxi, mini, thresh)
dd.accum<-c()
for (i in 1:length(dd)){
#hmm, need a way to sum up over the year, starting anew for each year.
#this should do it
if (DOY[i]==1){
dd.accum.day=0
}
#the accumulation on day i is the degree day accumulation before
#plus the dd accumulated on that day
dd.accum.day<-dd.accum.day+dd[i]
#but if the degdays are accumulating before the startday, we want to forget them
if (DOY[i]<startday){
dd.accum.day=0
}
#add that day's accumulation to the vector
dd.accum<-c(dd.accum, dd.accum.day)
}
return (dd.accum)
}
weather$dd.accum<-accum.allen(weather$temp_max_cleaned, weather$temp_min_cleaned, 10, weather$DOY, start)
str(weather$DOY)
start<-1
weather$dd.accum<-accum.allen(weather$temp_max_cleaned, weather$temp_min_cleaned, 10, weather$DOY, start)
#and plot that thing to look for problems:
plot(weather$DOY, weather$dd.accum)
start<-150
weather$dd.accum<-accum.allen(weather$temp_max_cleaned, weather$temp_min_cleaned, 10, weather$DOY, start)
#and plot that thing to look for problems:
plot(weather$DOY, weather$dd.accum)
#looks good! victory!!!
start<-37
weather$dd.accum<-accum.allen(weather$temp_max_cleaned, weather$temp_min_cleaned, 10, weather$DOY, start)
#and plot that thing to look for problems:
plot(weather$DOY, weather$dd.accum)
#same sort of checks. Run the function for our data
start<-1
weather$dd.accum<-accum.allen(weather$temp_max_cleaned, weather$temp_min_cleaned, 10, weather$DOY, start)
#and plot that thing to look for problems:
plot(weather$DOY, weather$dd.accum)
#looks good! victory!!!
